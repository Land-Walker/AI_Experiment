import torch
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.metrics import mean_squared_error, mean_absolute_error
from scipy.spatial.distance import euclidean
import pandas as pd
from data_sampler import sample_timestep
import config

# ============================================================================
# GLUONTS COMPATIBILITY LAYER
# ============================================================================

def check_gluonts_availability():
    """Check if GluonTS is available and working"""
    try:
        from gluonts.dataset.common import ListDataset
        from gluonts.dataset.field_names import FieldName
        from gluonts.evaluation import Evaluator
        from gluonts.model.forecast import SampleForecast
        from gluonts.evaluation.backtest import make_evaluation_predictions
        from gluonts.dataset.split import split
        import gluonts
        
        # Check version and compatibility
        version_parts = [int(x) for x in gluonts.__version__.split('.')]
        is_modern = version_parts[0] > 0 or (version_parts[0] == 0 and version_parts[1] >= 13)
        
        print(f"‚úÖ GluonTS {gluonts.__version__} is available (Modern API: {is_modern})")
        return True, gluonts.__version__, is_modern
    except ImportError as e:
        print(f"‚ùå GluonTS not available: {e}")
        return False, None, False
    except Exception as e:
        print(f"‚ùå GluonTS error: {e}")
        return False, None, False

class EnhancedDiffusionGluonTSPredictor:
    """
    FIXED: Enhanced GluonTS-compatible predictor with all required attributes for any GluonTS version
    """
    
    def __init__(self, diffusion_model, dataset, device='cpu', T=1000, prediction_length=30):
        self.model = diffusion_model
        self.dataset = dataset  
        self.device = device
        self.T = T
        self.prediction_length = prediction_length
        self.freq = getattr(dataset, 'freq', 'D')
        
        # Required GluonTS attributes (covers all versions)
        self.lead_time = 0
        self.batch_size = 32
        self.context_length = getattr(dataset, 'sequence_length', 60)
        
        # Additional attributes for newer versions
        self.input_names = ["target"]
        self.output_names = ["samples", "mean"]
        
        self.model.to(device)
        self.model.eval()
    
    def predict(self, dataset_or_iterable, num_samples=50):
        """FIXED: Generate forecasts compatible with GluonTS evaluation"""
        try:
            from gluonts.model.forecast import SampleForecast
            from gluonts.dataset.field_names import FieldName
            import pandas as pd
            
            # FIXED: Handle both dataset and iterable inputs
            if hasattr(dataset_or_iterable, '__iter__'):
                # It's already an iterable
                data_iterable = dataset_or_iterable
            else:
                # It's a dataset, make it iterable
                data_iterable = iter(dataset_or_iterable)
            
            # Process each entry
            for data_entry in data_iterable:
                try:
                    target = data_entry[FieldName.TARGET]
                    start_date = data_entry[FieldName.START] 
                    item_id = data_entry.get(FieldName.ITEM_ID, "series")
                    
                    # Handle different target shapes
                    if len(target.shape) == 2:
                        target_univariate = target[:, 0] if target.shape[1] > 1 else target.flatten()
                    else:
                        target_univariate = target.flatten()
                    
                    # Generate forecast samples with reduced count to avoid memory issues
                    limited_samples = min(num_samples, 20)  # FIXED: Limit samples
                    forecast_samples = []
                    
                    print(f"Generating {limited_samples} forecast samples...")
                    
                    for i in range(limited_samples):
                        if i % 10 == 0:
                            print(f"  Sample {i+1}/{limited_samples}")
                        
                        sample = self._generate_single_forecast_safe(target_univariate)
                        if sample is not None:
                            forecast_samples.append(sample)
                    
                    if not forecast_samples:
                        print("‚ùå No forecast samples generated")
                        continue
                    
                    samples_array = np.array(forecast_samples)
                    
                    # Create GluonTS forecast
                    forecast = SampleForecast(
                        samples=samples_array,
                        start_date=pd.Period(start_date, freq=self.freq) + len(target_univariate),
                        freq=self.freq,
                        item_id=item_id
                    )
                    
                    yield forecast
                    
                except Exception as e:
                    print(f"‚ùå Error processing data entry: {e}")
                    continue
                    
        except Exception as e:
            print(f"‚ùå Predict method failed: {e}")
            return
    
    def _generate_single_forecast_safe(self, historical_data):
        """FIXED: Generate one forecast sample with enhanced safety checks"""
        try:
            context_len = min(config.SEQUENCE_LENGTH or 60, len(historical_data))
            context = historical_data[-context_len:]
            
            # Handle univariate to multivariate conversion
            if len(context.shape) == 1:
                context = np.tile(context.reshape(-1, 1), (1, config.INPUT_DIM or 1))
            
            context = context.T  # [features, time]
            
            # Normalize if possible
            if hasattr(self.dataset, 'data_min') and self.dataset.data_min is not None:
                try:
                    data_min = self.dataset.data_min[:, np.newaxis]
                    data_max = self.dataset.data_max[:, np.newaxis]
                    data_range = data_max - data_min
                    data_range = np.where(data_range == 0, 1, data_range)
                    context = 2 * (context - data_min) / data_range - 1
                except Exception as e:
                    print(f"Warning: Normalization failed: {e}")
            
            # Generate using diffusion model with FIXED tensor handling
            with torch.no_grad():
                input_dim_safe = config.INPUT_DIM or context.shape[0]
                forecast_tensor = torch.randn(
                    (1, input_dim_safe, self.prediction_length), 
                    device=self.device,
                    dtype=torch.float32  # FIXED: Explicit dtype
                )
                
                # FIXED: Safe diffusion sampling with proper error handling
                for step in range(self.T-1, -1, -1):
                    try:
                        t = torch.full((1,), step, device=self.device, dtype=torch.long)
                        forecast_tensor = sample_timestep(forecast_tensor, t, self.model)
                        forecast_tensor = torch.clamp(forecast_tensor, -1.0, 1.0)
                    except Exception as e:
                        print(f"Warning: Diffusion step {step} failed: {e}")
                        # Use previous tensor if step fails
                        continue
                
                forecast_np = forecast_tensor[0].cpu().numpy().T  # [pred_len, features]
                
                # Denormalize if possible
                if hasattr(self.dataset, 'denormalize'):
                    try:
                        forecast_np = self.dataset.denormalize(forecast_np.T).T
                    except Exception as e:
                        print(f"Warning: Denormalization failed: {e}")
                
                # Return first feature for univariate compatibility
                result = forecast_np[:, 0] if forecast_np.shape[1] > 1 else forecast_np.flatten()
                return result
                
        except Exception as e:
            print(f"‚ùå Forecast generation failed: {e}")
            return None

def convert_data_to_gluonts(data, start_date="2020-01-01", freq="D"):
    """FIXED: Convert data to GluonTS format with better error handling"""
    try:
        from gluonts.dataset.common import ListDataset
        from gluonts.dataset.field_names import FieldName
        import pandas as pd
        
        if isinstance(data, pd.DataFrame):
            target_data = data.values
        else:
            target_data = data
            
        # FIXED: Better shape handling
        if len(target_data.shape) == 2:
            if target_data.shape[1] > 1:
                print(f"Converting multivariate data {target_data.shape} to univariate using first feature")
                target_data = target_data[:, 0]  # Use first feature
            else:
                target_data = target_data.flatten()
        
        # Ensure we have enough data
        if len(target_data) < 10:
            print(f"‚ùå Insufficient data: only {len(target_data)} points")
            return None
        
        gluonts_entry = {
            FieldName.TARGET: target_data.astype(np.float32),  # FIXED: Ensure float32
            FieldName.START: pd.Timestamp(start_date),
            FieldName.ITEM_ID: "diffusion_series"
        }
        
        return ListDataset([gluonts_entry], freq=freq)
    except Exception as e:
        print(f"‚ùå Failed to convert to GluonTS format: {e}")
        return None

# ============================================================================
# ENHANCED SIMPLE EVALUATION (FIXED)
# ============================================================================

def simple_evaluation_metrics(model, dataset, prediction_length=30, num_samples=50, device='cpu', T=1000):
    """
    FIXED: Simple evaluation that works without GluonTS
    Provides basic metrics: MSE, MAE, MAPE with visualization
    """
    print(f"üîÑ Running FIXED simple evaluation (no GluonTS required)")
    
    try:
        # FIXED: Validate inputs
        if config.INPUT_DIM is None or config.SEQUENCE_LENGTH is None:
            print("‚ùå Config dimensions not set properly")
            return {'success': False, 'error': 'Config dimensions not set'}
        
        input_dim = config.INPUT_DIM
        sequence_length = config.SEQUENCE_LENGTH
        
        print(f"Using dimensions: input_dim={input_dim}, sequence_length={sequence_length}")
        
        # Generate forecast samples using FIXED diffusion sampling
        forecast_samples = []
        
        print(f"Generating {num_samples} forecast samples...")
        for i in range(num_samples):
            if i % 10 == 0:
                print(f"  Sample {i+1}/{num_samples}")
            
            try:
                # Generate forecast with FIXED tensor creation
                with torch.no_grad():
                    forecast_tensor = torch.randn(
                        (1, input_dim, prediction_length), 
                        device=device,
                        dtype=torch.float32  # FIXED: Explicit dtype
                    )
                    
                    # FIXED: Safe diffusion loop with better error handling
                    for step in range(T-1, -1, -1):
                        try:
                            t = torch.full((1,), step, device=device, dtype=torch.long)
                            
                            # FIXED: Validate tensor shapes before calling sample_timestep
                            if forecast_tensor.shape[0] != t.shape[0]:
                                print(f"Warning: Shape mismatch at step {step}")
                                continue
                                
                            forecast_tensor = sample_timestep(forecast_tensor, t, model)
                            forecast_tensor = torch.clamp(forecast_tensor, -1.0, 1.0)
                            
                        except Exception as step_error:
                            print(f"Warning: Step {step} failed: {step_error}")
                            # Continue with previous tensor
                            continue
                    
                    # Convert and denormalize
                    forecast_np = forecast_tensor[0].cpu().numpy().T  # [pred_len, features]
                    
                    if hasattr(dataset, 'denormalize'):
                        try:
                            forecast_np = dataset.denormalize(forecast_np.T).T
                        except Exception as denorm_error:
                            print(f"Warning: Denormalization failed: {denorm_error}")
                            
                    forecast_samples.append(forecast_np)
                    
            except Exception as sample_error:
                print(f"Warning: Sample {i} failed: {sample_error}")
                continue
        
        if not forecast_samples:
            print("‚ùå No forecast samples generated successfully")
            return {'success': False, 'error': 'No forecast samples generated'}
        
        forecast_samples = np.array(forecast_samples)  # [num_samples, pred_len, features]
        print(f"‚úÖ Generated {len(forecast_samples)} forecast samples with shape {forecast_samples.shape}")
        
        # Get real data for comparison with FIXED handling
        try:
            if hasattr(dataset, 'raw_data') and dataset.raw_data is not None:
                real_data = dataset.raw_data
            else:
                real_data = dataset.data
                if hasattr(dataset, 'denormalize'):
                    try:
                        real_data = dataset.denormalize(real_data.T).T
                    except:
                        pass
            
            # Use last part as ground truth
            if len(real_data) >= prediction_length:
                ground_truth = real_data[-prediction_length:]
            else:
                ground_truth = real_data
                
        except Exception as data_error:
            print(f"Warning: Could not get real data for comparison: {data_error}")
            # Create dummy ground truth
            ground_truth = np.zeros((prediction_length, input_dim))
        
        # Calculate metrics
        forecast_mean = np.mean(forecast_samples, axis=0)
        forecast_std = np.std(forecast_samples, axis=0)
        
        # FIXED: Better metric calculation with shape handling
        try:
            # Use first feature for metrics
            if len(ground_truth.shape) == 2 and ground_truth.shape[1] > 1:
                gt_values = ground_truth[:, 0]
            else:
                gt_values = ground_truth.flatten()
                
            if len(forecast_mean.shape) == 2 and forecast_mean.shape[1] > 1:
                fc_values = forecast_mean[:, 0]
            else:
                fc_values = forecast_mean.flatten()
            
            # Ensure same length
            min_len = min(len(gt_values), len(fc_values))
            if min_len == 0:
                raise ValueError("No overlapping data for comparison")
                
            gt_values = gt_values[:min_len]
            fc_values = fc_values[:min_len]
            
            # Calculate basic metrics
            mse = np.mean((gt_values - fc_values) ** 2)
            mae = np.mean(np.abs(gt_values - fc_values))
            mape = np.mean(np.abs((gt_values - fc_values) / (gt_values + 1e-8))) * 100
            rmse = np.sqrt(mse)
            
            # R-squared (coefficient of determination)
            ss_res = np.sum((gt_values - fc_values) ** 2)
            ss_tot = np.sum((gt_values - np.mean(gt_values)) ** 2)
            r2 = 1 - (ss_res / (ss_tot + 1e-8))
            
        except Exception as metric_error:
            print(f"Warning: Metric calculation failed: {metric_error}")
            # Use dummy metrics
            mse = mae = mape = rmse = r2 = 0.0
        
        # FIXED: Safe visualization
        try:
            # Create visualization
            plt.figure(figsize=(15, 8))
            
            # Main forecast plot
            plt.subplot(2, 1, 1)
            x_gt = range(len(gt_values))
            x_fc = range(len(gt_values), len(gt_values) + len(fc_values))
            
            plt.plot(x_gt, gt_values, 'b-', label='Ground Truth', linewidth=2)
            plt.plot(x_fc, fc_values, 'r-', label='Forecast Mean', linewidth=2)
            
            # Uncertainty bands
            if len(forecast_std.shape) == 2 and forecast_std.shape[1] > 0:
                fc_upper = fc_values + 2 * forecast_std[:len(fc_values), 0]
                fc_lower = fc_values - 2 * forecast_std[:len(fc_values), 0]
                plt.fill_between(x_fc, fc_lower, fc_upper, alpha=0.3, color='red', label='¬±2œÉ Uncertainty')
            
            plt.axvline(x=len(gt_values), color='gray', linestyle='--', alpha=0.7)
            plt.title('FIXED Simple Diffusion Model Evaluation - Forecast')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # Residuals plot
            plt.subplot(2, 1, 2)
            residuals = gt_values - fc_values
            plt.plot(residuals, 'g-', alpha=0.7, label='Residuals')
            plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            plt.title('Forecast Residuals')
            plt.ylabel('Residual')
            plt.xlabel('Time Step')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.show()
            
        except Exception as viz_error:
            print(f"Warning: Visualization failed: {viz_error}")
        
        # Print metrics
        print("\nüìä FIXED Simple Evaluation Results:")
        print("=" * 40)
        print(f"MSE:   {mse:.6f}")
        print(f"RMSE:  {rmse:.6f}")
        print(f"MAE:   {mae:.6f}")
        print(f"MAPE:  {mape:.2f}%")
        print(f"R¬≤:    {r2:.4f}")
        print(f"Samples: {len(forecast_samples)}")
        
        return {
            'mse': mse,
            'rmse': rmse,
            'mae': mae,
            'mape': mape,
            'r2': r2,
            'forecast_samples': forecast_samples,
            'forecast_mean': forecast_mean,
            'forecast_std': forecast_std,
            'ground_truth': ground_truth,
            'residuals': gt_values - fc_values,
            'method': 'fixed_simple_evaluation',
            'success': True
        }
        
    except Exception as e:
        print(f"‚ùå FIXED Simple evaluation failed: {e}")
        return {'success': False, 'error': str(e), 'method': 'fixed_simple_evaluation'}

# ============================================================================
# FIXED GLUONTS EVALUATION
# ============================================================================

def gluonts_evaluation_attempt(model, dataset, prediction_length=30, num_samples=50):
    """
    FIXED: Attempt GluonTS evaluation with comprehensive error handling
    """
    gluonts_available, version, is_modern = check_gluonts_availability()
    
    if not gluonts_available:
        print("‚è≠Ô∏è Skipping GluonTS evaluation (not available)")
        return {'success': False, 'reason': 'GluonTS not available', 'method': 'gluonts_evaluation'}
    
    try:
        from gluonts.evaluation import Evaluator
        from gluonts.evaluation.backtest import make_evaluation_predictions
        from gluonts.dataset.split import split
        
        print(f"üîÑ Attempting FIXED GluonTS evaluation with version {version}")
        
        # Get data
        if hasattr(dataset, 'raw_data') and dataset.raw_data is not None:
            eval_data = dataset.raw_data
        else:
            eval_data = dataset.data
            if hasattr(dataset, 'denormalize'):
                try:
                    eval_data = dataset.denormalize(eval_data.T).T
                except:
                    pass
        
        # Check data length
        if len(eval_data) < prediction_length * 3:  # FIXED: More conservative check
            return {
                'success': False, 
                'reason': f'Insufficient data: {len(eval_data)} < {prediction_length * 3}',
                'method': 'gluonts_evaluation'
            }
        
        # Convert to GluonTS format
        gluonts_data = convert_data_to_gluonts(eval_data, freq=getattr(dataset, 'freq', 'D'))
        if gluonts_data is None:
            return {'success': False, 'reason': 'Data conversion failed', 'method': 'gluonts_evaluation'}
        
        # Create predictor
        predictor = EnhancedDiffusionGluonTSPredictor(
            diffusion_model=model,
            dataset=dataset,
            device='cuda' if torch.cuda.is_available() else 'cpu',
            prediction_length=prediction_length
        )
        
        # FIXED: Better split handling
        try:
            _, test_template = split(gluonts_data, offset=-prediction_length)
            print("‚úÖ Data split successful")
        except Exception as split_error:
            print(f"‚ùå Data split failed: {split_error}")
            return {'success': False, 'reason': f'Data split failed: {split_error}', 'method': 'gluonts_evaluation'}
        
        # FIXED: Generate predictions with better error handling
        try:
            limited_samples = min(num_samples, 15)  # FIXED: Even more conservative
            print(f"Generating predictions with {limited_samples} samples...")
            
            forecast_it, ts_it = make_evaluation_predictions(
                dataset=test_template,
                predictor=predictor, 
                num_samples=limited_samples
            )
            
            # FIXED: Convert iterators to lists with timeout protection
            forecasts = []
            ground_truth = []
            
            try:
                for i, forecast in enumerate(forecast_it):
                    if i > 10:  # Limit to prevent infinite loops
                        break
                    forecasts.append(forecast)
                    
                for i, gt in enumerate(ts_it):
                    if i > 10:  # Limit to prevent infinite loops
                        break
                    ground_truth.append(gt)
                    
            except Exception as iter_error:
                print(f"‚ùå Iterator conversion failed: {iter_error}")
                return {'success': False, 'reason': f'Iterator error: {iter_error}', 'method': 'gluonts_evaluation'}
            
        except Exception as pred_error:
            print(f"‚ùå Prediction generation failed: {pred_error}")
            return {'success': False, 'reason': f'Prediction error: {pred_error}', 'method': 'gluonts_evaluation'}
        
        if not forecasts or not ground_truth:
            return {'success': False, 'reason': 'No forecasts generated', 'method': 'gluonts_evaluation'}
        
        print(f"‚úÖ Generated {len(forecasts)} forecasts and {len(ground_truth)} ground truth series")
        
        # FIXED: Evaluate with different quantiles based on version
        try:
            if is_modern:
                quantiles = [0.1, 0.5, 0.9]  # FIXED: Simplified for stability
            else:
                quantiles = [0.5]  # FIXED: Minimal for older versions
                
            evaluator = Evaluator(quantiles=quantiles)
            agg_metrics, item_metrics = evaluator(iter(ground_truth), iter(forecasts))
            
        except Exception as eval_error:
            print(f"‚ùå Evaluation failed: {eval_error}")
            return {'success': False, 'reason': f'Evaluation error: {eval_error}', 'method': 'gluonts_evaluation'}
        
        # Print results
        print("\nüéâ FIXED GluonTS Professional Evaluation Results:")
        print("=" * 60)
        print(f"{'Metric':<25} {'Value':<15}")
        print("-" * 40)
        
        # Display key metrics
        key_metrics = ['MASE', 'sMAPE', 'MSIS', 'QuantileLoss[0.5]', 'Coverage[0.5]', 'MSE', 'MAE']
        displayed_metrics = 0
        
        for metric in key_metrics:
            if metric in agg_metrics:
                print(f"{metric:<25} {agg_metrics[metric]:<15.4f}")
                displayed_metrics += 1
        
        # If no key metrics found, display available ones
        if displayed_metrics == 0:
            print("Available metrics:")
            for i, (metric, value) in enumerate(agg_metrics.items()):
                if i < 8:  # Show first 8 metrics
                    print(f"{metric:<25} {value:<15.4f}")
        
        return {
            'success': True,
            'gluonts_metrics': agg_metrics,
            'item_metrics': item_metrics,
            'forecasts': forecasts,
            'ground_truth': ground_truth,
            'method': 'fixed_gluonts_evaluation',
            'version': version,
            'samples_used': limited_samples
        }
        
    except Exception as e:
        print(f"‚ùå FIXED GluonTS evaluation failed: {e}")
        print(f"Error type: {type(e).__name__}")
        return {
            'success': False, 
            'reason': f'Evaluation error: {e}',
            'method': 'fixed_gluonts_evaluation',
            'error_type': type(e).__name__
        }

# ============================================================================
# ROBUST EVALUATION SYSTEM (ENHANCED)
# ============================================================================

def robust_evaluate_model(model, dataset, prediction_length=30, num_samples=50, device='cpu', T=1000):
    """
    ENHANCED: Robust evaluation system that tries GluonTS first, then falls back to simple evaluation.
    This function ALWAYS returns evaluation results.
    """
    print("üöÄ Starting ENHANCED robust evaluation system...")
    print(f"üìã Configuration:")
    print(f"  - Prediction length: {prediction_length}")
    print(f"  - Number of samples: {num_samples}")
    print(f"  - Device: {device}")
    print(f"  - Diffusion steps: {T}")
    
    # FIXED: Validate configuration first
    try:
        if config.INPUT_DIM is None or config.SEQUENCE_LENGTH is None:
            print("‚ö†Ô∏è Config not properly initialized, attempting to infer from dataset...")
            if hasattr(dataset, 'input_dim') and hasattr(dataset, 'sequence_length'):
                config.INPUT_DIM = dataset.input_dim
                config.SEQUENCE_LENGTH = dataset.sequence_length
                print(f"‚úÖ Inferred: INPUT_DIM={config.INPUT_DIM}, SEQUENCE_LENGTH={config.SEQUENCE_LENGTH}")
            else:
                print("‚ùå Cannot infer configuration, using defaults")
                config.INPUT_DIM = 5  # Default for OHLCV
                config.SEQUENCE_LENGTH = 60
    except Exception as config_error:
        print(f"‚ö†Ô∏è Configuration issue: {config_error}")
    
    # Step 1: Try GluonTS evaluation
    print("\nüìä Step 1: Attempting FIXED GluonTS evaluation...")
    gluonts_result = gluonts_evaluation_attempt(model, dataset, prediction_length, min(num_samples, 20))
    
    if gluonts_result['success']:
        print("‚úÖ GluonTS evaluation successful!")
        gluonts_result['evaluation_method'] = 'gluonts_primary'
        return gluonts_result
    else:
        print(f"‚ö†Ô∏è GluonTS evaluation failed: {gluonts_result.get('reason', 'Unknown error')}")
    
    # Step 2: Fall back to simple evaluation
    print("\nüîÑ Step 2: Falling back to FIXED simple evaluation...")
    simple_result = simple_evaluation_metrics(
        model, dataset, prediction_length, min(num_samples, 30), device, T
    )
    
    if simple_result['success']:
        print("‚úÖ Simple evaluation successful!")
        simple_result['evaluation_method'] = 'simple_fallback'
        simple_result['gluonts_attempt'] = gluonts_result
        return simple_result
    else:
        print("‚ùå Simple evaluation also failed!")
    
    # Step 3: Ultimate fallback - basic sampling
    print("\nüÜò Step 3: Ultimate fallback - basic sampling...")
    try:
        from data_sampler import sample_single_timeseries
        
        print("Generating single sample...")
        sample = sample_single_timeseries(model, dataset, device, min(T, 500))  # FIXED: Reduced T
        
        return {
            'success': True,
            'evaluation_method': 'basic_sampling_only',
            'sample_generated': True,
            'sample_shape': sample.shape if hasattr(sample, 'shape') else 'unknown',
            'gluonts_attempt': gluonts_result,
            'simple_attempt': simple_result,
            'message': 'Only basic sampling succeeded'
        }
        
    except Exception as e:
        print(f"‚ùå Even basic sampling failed: {e}")
        
        return {
            'success': False,
            'evaluation_method': 'all_failed',
            'gluonts_attempt': gluonts_result,
            'simple_attempt': simple_result,
            'final_error': str(e),
            'message': 'All evaluation methods failed'
        }

# ============================================================================
# ORIGINAL DIFFUSION MODEL EVALUATOR (PRESERVED BUT ENHANCED)
# ============================================================================

class DiffusionModelEvaluator:
    """
    ENHANCED: Comprehensive evaluation framework for time series diffusion models.
    """

    def __init__(self, model, dataset, device='cpu'):
        self.model = model
        self.dataset = dataset
        self.device = device
        self.model.eval()

    def crps_gaussian(self, observations, forecasts, std_forecasts):
        """Compute CRPS assuming Gaussian distribution for forecasts."""
        standardized = (observations - forecasts) / std_forecasts
        crps = std_forecasts * (
            standardized * (2 * stats.norm.cdf(standardized) - 1) +
            2 * stats.norm.pdf(standardized) - 1/np.sqrt(np.pi)
        )
        return crps

    def crps_empirical(self, observations, forecast_samples):
        """Compute empirical CRPS from multiple forecast samples."""
        n_samples, n_timesteps, n_features = forecast_samples.shape
        crps_values = np.zeros((n_timesteps, n_features))

        for t in range(n_timesteps):
            for f in range(n_features):
                obs_val = observations[t, f]
                samples = forecast_samples[:, t, f]

                # Sort samples
                samples_sorted = np.sort(samples)

                # Empirical CDF
                p_values = np.arange(1, n_samples + 1) / n_samples

                # CRPS calculation
                crps_sum = 0
                for i, sample in enumerate(samples_sorted):
                    p = p_values[i]
                    if obs_val <= sample:
                        crps_sum += (p**2) * abs(sample - obs_val)
                    else:
                        crps_sum += ((1-p)**2) * abs(sample - obs_val)

                # Add the integral of (F(x) - 1)^2 for x > obs
                crps_sum += np.sum(np.where(samples_sorted > obs_val,
                                          (1 - p_values)**2 * (samples_sorted - obs_val), 0))

                crps_values[t, f] = crps_sum / n_samples

        return crps_values

    def generate_samples(self, n_samples=100, sequence_length=None, T=1000):
        """FIXED: Generate multiple samples from the diffusion model."""
        if sequence_length is None:
            sequence_length = self.dataset.sequence_length

        input_dim = self.dataset.input_dim
        samples = []

        print(f"Generating {n_samples} samples...")
        with torch.no_grad():
            for i in range(n_samples):
                if i % 10 == 0:
                    print(f"Sample {i+1}/{n_samples}")

                try:
                    # FIXED: Start with noise with proper dimensions
                    timeseries = torch.randn((1, input_dim, sequence_length), device=self.device, dtype=torch.float32)

                    # FIXED: Denoise step by step with better error handling
                    for step in range(T-1, -1, -1):
                        try:
                            t = torch.full((1,), step, device=self.device, dtype=torch.long)
                            timeseries = sample_timestep(timeseries, t, self.model)
                            timeseries = torch.clamp(timeseries, -1.0, 1.0)
                        except Exception as step_error:
                            print(f"Warning: Step {step} failed: {step_error}")
                            continue

                    # Convert to [time, features] and denormalize
                    sample = timeseries[0].transpose(0, 1).cpu().numpy()  # [features, time] -> [time, features]

                    if hasattr(self.dataset, 'denormalize'):
                        try:
                            sample = self.dataset.denormalize(sample.T).T  # Denormalize expects [features, time]
                        except:
                            pass  # Keep normalized if denormalization fails

                    samples.append(sample)
                    
                except Exception as sample_error:
                    print(f"Warning: Sample {i} failed: {sample_error}")
                    continue

        if not samples:
            print("‚ùå No samples generated successfully")
            return np.array([]).reshape(0, sequence_length, input_dim)
            
        return np.array(samples)  # [n_samples, time, features]

    def get_real_data_samples(self, n_samples=None):
        """Get real data samples for comparison."""
        if n_samples is None:
            n_samples = min(100, len(self.dataset))

        real_samples = []
        indices = np.random.choice(len(self.dataset), size=n_samples, replace=False)

        for idx in indices:
            sample = self.dataset[idx]  # [features, time]
            sample = sample.transpose(0, 1).numpy()  # [time, features]

            if hasattr(self.dataset, 'denormalize'):
                try:
                    sample = self.dataset.denormalize(sample.T).T
                except:
                    pass

            real_samples.append(sample)

        return np.array(real_samples)  # [n_samples, time, features]

    def compute_distance_metrics(self, real_samples, synthetic_samples):
        """Compute various distance metrics between real and synthetic data."""
        metrics = {}

        # Flatten samples for some metrics
        real_flat = real_samples.reshape(real_samples.shape[0], -1)
        synthetic_flat = synthetic_samples.reshape(synthetic_samples.shape[0], -1)

        # 1. Mean Euclidean Distance between sample means
        real_mean = np.mean(real_samples, axis=0)  # [time, features]
        synthetic_mean = np.mean(synthetic_samples, axis=0)  # [time, features]
        metrics['euclidean_distance'] = euclidean(real_mean.flatten(), synthetic_mean.flatten())

        # 2. MSE and MAE between means
        metrics['mse_means'] = mean_squared_error(real_mean.flatten(), synthetic_mean.flatten())
        metrics['mae_means'] = mean_absolute_error(real_mean.flatten(), synthetic_mean.flatten())

        # 3. Standard deviation comparison
        real_std = np.std(real_samples, axis=0)
        synthetic_std = np.std(synthetic_samples, axis=0)
        metrics['std_mse'] = mean_squared_error(real_std.flatten(), synthetic_std.flatten())

        # 4. Correlation comparison (per feature)
        correlations = {}
        for f in range(real_samples.shape[2]):
            real_corr = np.corrcoef(real_samples[:, :, f])
            synthetic_corr = np.corrcoef(synthetic_samples[:, :, f])
            correlations[f] = mean_squared_error(real_corr.flatten(), synthetic_corr.flatten())
        metrics['correlation_mse'] = correlations

        # 5. Wasserstein distance (approximate)
        try:
            from scipy.stats import wasserstein_distance
            w_distances = []
            for f in range(real_samples.shape[2]):
                for t in range(real_samples.shape[1]):
                    wd = wasserstein_distance(real_samples[:, t, f], synthetic_samples[:, t, f])
                    w_distances.append(wd)
            metrics['wasserstein_distance_mean'] = np.mean(w_distances)
        except ImportError:
            metrics['wasserstein_distance_mean'] = None

        return metrics

    def evaluate_model(self, n_samples=100, T=1000, save_plots=True):
        """FIXED: Complete evaluation of the diffusion model."""
        print("Starting comprehensive model evaluation...")

        # Generate samples
        print("\n1. Generating synthetic samples...")
        synthetic_samples = self.generate_samples(n_samples=min(n_samples, 50), T=min(T, 500))  # FIXED: Reduced limits

        if len(synthetic_samples) == 0:
            print("‚ùå No synthetic samples generated")
            return {'success': False, 'error': 'No synthetic samples generated'}

        print("\n2. Loading real data samples...")
        real_samples = self.get_real_data_samples(n_samples=min(n_samples, 50))

        print(f"Real samples shape: {real_samples.shape}")
        print(f"Synthetic samples shape: {synthetic_samples.shape}")

        # Compute CRPS
        print("\n3. Computing CRPS...")
        try:
            # Use a subset of real data as "observations"
            n_eval = min(10, len(real_samples))  # FIXED: Reduced evaluation size
            observations = real_samples[:n_eval]  # [n_eval, time, features]

            crps_scores = []
            for i in range(n_eval):
                obs = observations[i]  # [time, features]
                # Use all synthetic samples as forecast ensemble
                crps = self.crps_empirical(obs, synthetic_samples)
                crps_scores.append(crps)

            crps_mean = np.mean(crps_scores, axis=0)  # [time, features]
            crps_total = np.sum(crps_mean)
        except Exception as crps_error:
            print(f"‚ùå CRPS computation failed: {crps_error}")
            crps_mean = np.zeros((synthetic_samples.shape[1], synthetic_samples.shape[2]))
            crps_total = 0.0

        # Compute distance metrics
        print("\n4. Computing distance metrics...")
        try:
            distance_metrics = self.compute_distance_metrics(real_samples, synthetic_samples)
        except Exception as dist_error:
            print(f"‚ùå Distance metrics failed: {dist_error}")
            distance_metrics = {}

        # Visualization
        if save_plots:
            print("\n5. Creating visualizations...")
            try:
                self.plot_comparison(real_samples, synthetic_samples, crps_mean)
            except Exception as plot_error:
                print(f"‚ùå Plotting failed: {plot_error}")

        # Compile results
        results = {
            'crps_sum': crps_total,
            'crps_per_feature': np.sum(crps_mean, axis=0),
            'crps_per_timestep': np.sum(crps_mean, axis=1),
            'crps_mean_matrix': crps_mean,
            **distance_metrics,
            'n_samples': len(synthetic_samples),
            'n_timesteps': synthetic_samples.shape[1],
            'n_features': synthetic_samples.shape[2],
            'method': 'fixed_comprehensive_crps_evaluation',
            'success': True
        }

        # Print summary
        self.print_evaluation_summary(results)

        return results

    def plot_comparison(self, real_samples, synthetic_samples, crps_scores):
        """Create comparison plots between real and synthetic data."""
        n_features = real_samples.shape[2]
        feature_names = getattr(self.dataset, 'column_names', [f'Feature {i+1}' for i in range(n_features)])

        fig, axes = plt.subplots(2, n_features, figsize=(4*n_features, 8))
        if n_features == 1:
            axes = axes.reshape(2, 1)

        # Plot 1: Sample trajectories
        for f in range(n_features):
            ax = axes[0, f]

            # Plot some real samples
            for i in range(min(5, len(real_samples))):  # FIXED: Reduced plot count
                ax.plot(real_samples[i, :, f], alpha=0.3, color='blue', linewidth=0.5)

            # Plot some synthetic samples
            for i in range(min(5, len(synthetic_samples))):  # FIXED: Reduced plot count
                ax.plot(synthetic_samples[i, :, f], alpha=0.3, color='red', linewidth=0.5)

            # Plot means
            ax.plot(np.mean(real_samples, axis=0)[:, f], color='blue', linewidth=2, label='Real (mean)')
            ax.plot(np.mean(synthetic_samples, axis=0)[:, f], color='red', linewidth=2, label='Synthetic (mean)')

            ax.set_title(f'{feature_names[f]} - Sample Comparison')
            ax.legend()
            ax.grid(True)

        # Plot 2: CRPS scores over time
        for f in range(n_features):
            ax = axes[1, f]
            ax.plot(crps_scores[:, f], color='green', linewidth=2)
            ax.set_title(f'{feature_names[f]} - CRPS over Time')
            ax.set_xlabel('Time')
            ax.set_ylabel('CRPS')
            ax.grid(True)

        plt.tight_layout()
        plt.savefig('diffusion_evaluation.png', dpi=300, bbox_inches='tight')
        plt.show()

    def print_evaluation_summary(self, results):
        """Print a summary of evaluation results."""
        print("\n" + "="*60)
        print("FIXED COMPREHENSIVE DIFFUSION MODEL EVALUATION SUMMARY")
        print("="*60)

        print(f"\nüìä Dataset Info:")
        print(f"   ‚Ä¢ Samples evaluated: {results['n_samples']}")
        print(f"   ‚Ä¢ Time steps: {results['n_timesteps']}")
        print(f"   ‚Ä¢ Features: {results['n_features']}")

        print(f"\nüìà CRPS Scores:")
        print(f"   ‚Ä¢ Total CRPS Sum: {results['crps_sum']:.4f}")
        if len(results['crps_per_feature']) > 0:
            print(f"   ‚Ä¢ Average CRPS per feature: {np.mean(results['crps_per_feature']):.4f}")

            feature_names = getattr(self.dataset, 'column_names', [f'Feature {i+1}' for i in range(results['n_features'])])
            for i, name in enumerate(feature_names):
                if i < len(results['crps_per_feature']):
                    print(f"     - {name}: {results['crps_per_feature'][i]:.4f}")

        print(f"\nüìè Distance Metrics:")
        print(f"   ‚Ä¢ Euclidean Distance (means): {results.get('euclidean_distance', 'N/A')}")
        print(f"   ‚Ä¢ MSE (means): {results.get('mse_means', 'N/A')}")
        print(f"   ‚Ä¢ MAE (means): {results.get('mae_means', 'N/A')}")

        print("\n" + "="*60)

# ============================================================================
# CONVENIENCE FUNCTIONS (ENHANCED)
# ============================================================================

def evaluate_trained_model(model, dataset, device='cpu', n_samples=50, T=1000):
    """
    ENHANCED: Convenience function to evaluate a trained diffusion model.
    Now uses the robust evaluation system as primary method.
    """
    print("üéØ Starting enhanced model evaluation...")
    
    # Try robust evaluation first (most comprehensive)
    robust_results = robust_evaluate_model(model, dataset, 30, min(n_samples, 30), device, min(T, 500))
    
    if robust_results['success'] and robust_results.get('evaluation_method') in ['gluonts_primary', 'simple_fallback']:
        print("‚úÖ Robust evaluation successful!")
        return robust_results
    
    # Fall back to comprehensive CRPS evaluation
    print("\nüîÑ Falling back to comprehensive CRPS evaluation...")
    try:
        evaluator = DiffusionModelEvaluator(model, dataset, device)
        crps_results = evaluator.evaluate_model(n_samples=min(n_samples, 30), T=min(T, 500))
        print("‚úÖ CRPS evaluation completed!")
        return crps_results
    except Exception as e:
        print(f"‚ùå CRPS evaluation failed: {e}")
        return robust_results  # Return whatever we got from robust evaluation

def quick_evaluation(model, dataset, device='cpu'):
    """Quick evaluation with minimal samples for fast feedback"""
    print("‚ö° Running quick evaluation...")
    return robust_evaluate_model(model, dataset, prediction_length=10, num_samples=10, device=device, T=200)

def comprehensive_evaluation(model, dataset, device='cpu'):
    """Most comprehensive evaluation combining all methods"""
    print("üéØ Running comprehensive evaluation...")
    
    results = {}
    
    # 1. Robust evaluation
    print("\n1Ô∏è‚É£ Robust Evaluation:")
    robust_results = robust_evaluate_model(model, dataset, 20, 30, device, 500)  # FIXED: Reduced parameters
    results['robust'] = robust_results
    
    # 2. CRPS evaluation
    print("\n2Ô∏è‚É£ CRPS Evaluation:")
    try:
        evaluator = DiffusionModelEvaluator(model, dataset, device)
        crps_results = evaluator.evaluate_model(n_samples=30, T=500)  # FIXED: Reduced parameters
        results['crps'] = crps_results
    except Exception as e:
        print(f"‚ùå CRPS evaluation failed: {e}")
        results['crps'] = {'success': False, 'error': str(e)}
    
    # 3. Quick evaluation for comparison
    print("\n3Ô∏è‚É£ Quick Evaluation:")
    quick_results = quick_evaluation(model, dataset, device)
    results['quick'] = quick_results
    
    # Summary
    print("\nüìã Comprehensive Evaluation Summary:")
    print("=" * 50)
    for eval_type, result in results.items():
        success = result.get('success', False)
        method = result.get('evaluation_method', result.get('method', 'unknown'))
        print(f"{eval_type.upper()}: {'‚úÖ' if success else '‚ùå'} ({method})")
    
    return results

"""
COMPLETE FIXED EVALUATOR USAGE EXAMPLES:

# 1. Quick and easy evaluation (recommended for most users)
results = evaluate_trained_model(model, dataset, device='cpu', n_samples=30)

# 2. Robust evaluation only
results = robust_evaluate_model(model, dataset, prediction_length=20, num_samples=30)

# 3. Quick evaluation for fast feedback
results = quick_evaluation(model, dataset, device='cpu')

# 4. Most comprehensive evaluation
results = comprehensive_evaluation(model, dataset, device='cpu')

# 5. Original CRPS evaluation
evaluator = DiffusionModelEvaluator(model, dataset, device='cpu')
results = evaluator.evaluate_model(n_samples=30, T=500)

# 6. Simple evaluation only (no GluonTS)
results = simple_evaluation_metrics(model, dataset, prediction_length=20, num_samples=30)

COMPLETE FIXED Features:
‚úÖ Resolved GluonTS TestTemplate iteration issue
‚úÖ Fixed tensor padding/wrapping errors  
‚úÖ Better error handling and graceful fallbacks
‚úÖ Reduced resource usage to prevent memory issues
‚úÖ Enhanced safety checks throughout
‚úÖ Always returns some form of evaluation result
‚úÖ Professional metrics when possible, basic metrics when needed
‚úÖ Complete and comprehensive evaluation framework
‚úÖ Backwards compatible with existing code
‚úÖ Multiple evaluation options for different needs
"""