# PRESERVED: Your exact imports
from torch.optim import Adam
import torch

from data_loader import *
from forward_process import *
from model import *
from data_sampler import *
from evaluator import *

# ============================================================================
# STEP 1: DATA LOADING AND INITIAL EXPLORATION
# ============================================================================

# PRESERVED: Your exact data loading code
# Data Loading
yf_df = load_yf_data()  # Downloads S&P 500 data from Yahoo Finance (default: ^GSPC from 2024-01-01)
show_yf_data(yf_df)     # Display basic price chart for visual inspection

# PRESERVED: Your exact dataset creation code with all options
# Option 1: Use your yfDf with selected columns
dataset, dataloader, info = load_timeseries_dataset(
    yf_dataframe=yf_df,  # Your existing variable
    use_columns=['Open', 'High', 'Low', 'Close'],  # Exclude Volume for cleaner data
    sequence_length=60  # 60-day sequences
)

# Option 2: Use all OHLCV data from your yfDf (auto-detects numeric columns)
# dataset, dataloader, info = load_timeseries_dataset(yf_dataframe=yfDf, sequence_length=30)

# Option 3: Just closing prices from your yfDf
# dataset, dataloader, info = load_timeseries_dataset(yf_dataframe=yfDf, use_columns=['Close'], sequence_length=100)

# PRESERVED: Your exact information display
# Print dataset information
print("Dataset Information:")
for key, value in info.items():
    print(f"  {key}: {value}")

# ============================================================================
# STEP 2: DATA SAMPLING AND VERIFICATION
# ============================================================================

# PRESERVED: Your exact sampling code with your comment style
# ---------------------------------------------------------------------------------------------------------------------------
# Sampling
# Show a sample
sample_batch = next(iter(dataloader))  # Get first batch from dataloader
print(f"\nSample batch shape: {sample_batch.shape}")  # Should be [batch_size, features, time]
show_timeseries_sample(sample_batch, dataset, "Stock Price Time Series")

# PRESERVED: Your exact duplicate dataset creation (maintaining your structure)
 # Option 1: Use your yfDf with selected columns
dataset, dataloader, info = load_timeseries_dataset(
    yf_dataframe=yf_df,  # Your existing variable
    use_columns=['Open', 'High', 'Low', 'Close'],  # Exclude Volume for cleaner data
    sequence_length=60  # 60-day sequences
)

# Option 2: Use all OHLCV data from your yfDf (auto-detects numeric columns)
# dataset, dataloader, info = load_timeseries_dataset(yf_dataframe=yfDf, sequence_length=30)

# Option 3: Just closing prices from your yfDf
# dataset, dataloader, info = load_timeseries_dataset(yf_dataframe=yfDf, use_columns=['Close'], sequence_length=100)

# PRESERVED: Your exact duplicate information display
# Print dataset information
print("Dataset Information:")
for key, value in info.items():
    print(f"  {key}: {value}")

# PRESERVED: Your exact duplicate sample examination
# Show a sample
sample_batch = next(iter(dataloader))
print(f"\nSample batch shape: {sample_batch.shape}")
show_timeseries_sample(sample_batch, dataset, "Stock Price Time Series")

# ============================================================================
# STEP 3: MODEL CREATION AND ARCHITECTURE TESTING
# ============================================================================

# PRESERVED: Your exact modeling section with your comment style
# ---------------------------------------------------------------------------------------------------------------------------
# Modelling
# For time series with 4 features (OHLC) and sequence length 100
model = create_wavenet_model(input_dim=4)

# PRESERVED: Your exact model testing code
# Test forward pass
batch_size = 8
sequence_length = 100
x = torch.randn(batch_size, 4, sequence_length)  # [batch, features, time]
timestep = torch.randint(0, 1000, (batch_size,))

output = model(x, timestep)
print(f"Input shape: {x.shape}")
print(f"Output shape: {output.shape}")
print(f"Num params: {sum(p.numel() for p in model.parameters()):,}")

# Verify shapes match (essential for diffusion)
assert output.shape == x.shape, "Output shape must match input shape for diffusion!"
print("âœ… Shape verification passed!")

# ============================================================================
# STEP 4: TRAINING SETUP AND CONFIGURATION
# ============================================================================

# PRESERVED: Your exact training setup with your comment style
# ---------------------------------------------------------------------------------------------------------------------------
# Training setup
device = "cuda" if torch.cuda.is_available() else "cpu"  # Use GPU if available for faster training
model.to(device)  # Move model to selected device
optimizer = Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate 0.001
epochs = 100  # Try more!

# PRESERVED: Your exact tensor device movement code
# Move pre-calculated tensors to the correct device
betas = betas.to(device)
alphas = alphas.to(device)
alphas_cumprod = alphas_cumprod.to(device)
sqrt_alphas_cumprod = sqrt_alphas_cumprod.to(device)
sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod.to(device)
sqrt_recip_alphas = sqrt_recip_alphas.to(device)
posterior_variance = posterior_variance.to(device)

# PRESERVED: Your exact loss function with your comment
# Make sure you have the loss function (should work unchanged)
def get_loss(model, x_0, t):
    """
    Calculate the loss for diffusion training.
    This function should work unchanged for time series.
    """
    x_noisy, noise = forward_diffusion_sample(x_0, t, device)
    noise_pred = model(x_noisy, t)
    return F.mse_loss(noise, noise_pred)

# ============================================================================
# STEP 5: MAIN TRAINING LOOP
# ============================================================================

# PRESERVED: Your exact training section with your comment style
# ---------------------------------------------------------------------------------------------------------------------------
# Training
for epoch in range(epochs):
    for step, batch in enumerate(dataloader):
        # Clear gradients from previous iteration
        optimizer.zero_grad()

        # PRESERVED: Your exact batch handling code with comments
        # Move batch to device and get correct shape
        if isinstance(batch, list):
            x_0 = batch[0].to(device)  # Handle list of tensors
        else:
            x_0 = batch.to(device)     # Handle single tensor

        # x_0 should now be [batch, features, time] from the updated dataset
        # No need for reshaping since dataset now returns correct format

        # PRESERVED: Your exact timestep generation code with comment
        # Generate random timesteps
        t = torch.randint(0, T, (x_0.shape[0],), device=device).long()  # Use actual batch size

        # PRESERVED: Your exact loss calculation code with comment
        # Calculate loss (same as original)
        loss = get_loss(model, x_0, t)
        loss.backward()
        optimizer.step()

        # PRESERVED: Your exact logging code with comments
        # Logging and sampling (modified for time series)
        if epoch % 5 == 0 and step == 0:
            print(f"Epoch {epoch} | step {step:03d} Loss: {loss.item():.6f}")

            # PRESERVED: Your exact sampling code with comments
            # Sample and show time series instead of image
            model.eval()
            with torch.no_grad():
                # Pass the model to sample_single_timeseries
                sample_single_timeseries(model, dataset, device, T)
            model.train()

    # PRESERVED: Your exact epoch summary code with comment
    # Optional: Print epoch summary
    if epoch % 10 == 0:
        print(f"Completed epoch {epoch}/{epochs}")

print("Training completed!")

# ============================================================================
# STEP 6: MODEL SAVING AND CHECKPOINTING
# ============================================================================

# PRESERVED: Your exact model saving code with comments
# Optional: Save the trained model
torch.save({
    'epoch': epochs,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss.item(),
}, f'wavenet_diffusion_epoch_{epochs}.pth')

print(f"Model saved as 'wavenet_diffusion_epoch_{epochs}.pth'")

# PRESERVED: Your exact evaluation code
print("Starting evaluation...")
results = evaluate_trained_model(model, dataset, device=device, n_samples=50, T=T)

# ============================================================================
# PIPELINE EXPLANATION AND EDUCATIONAL CONTENT
# ============================================================================
"""
COMPLETE DIFFUSION MODEL PIPELINE EXPLAINED:

This script implements a full end-to-end pipeline for training diffusion models
on financial time series data. Here's what each step accomplishes:

STEP 1: DATA LOADING AND PREPROCESSING
- Downloads real S&P 500 data from Yahoo Finance
- Converts to normalized sequences suitable for diffusion training
- Creates sliding windows of 60-day sequences
- Normalizes to [-1, 1] range (required for diffusion models)

Key insight: Financial data needs careful preprocessing because:
- Raw prices have different scales (price vs volume)
- Time series have trends that can bias the model
- Diffusion models work best with normalized, stationary data

STEP 2: ARCHITECTURE SELECTION
- Uses WaveNet instead of U-Net for temporal data
- Dilated convolutions capture multi-scale patterns
- Skip connections preserve information across scales
- Time embeddings condition model on diffusion timestep

Why WaveNet for finance:
- Captures both short-term (daily) and long-term (monthly) patterns
- Efficient parallel training unlike RNNs
- Respects temporal ordering unlike standard CNNs
- Proven effective for sequential data (originally audio)

STEP 3: TRAINING PROCESS
- Implements DDPM (Denoising Diffusion Probabilistic Models) objective
- At each step: add noise to real data, train model to predict that noise
- Random timesteps ensure model learns all noise levels
- MSE loss between actual and predicted noise

Training efficiency comes from:
- Parallel processing across timesteps
- Precomputed diffusion parameters
- Stable gradient flow through residual connections

STEP 4: SAMPLING AND GENERATION
- Reverse process: start with noise, gradually denoise
- Model predicts noise at each step, subtract to get cleaner data
- Add controlled noise back (except final step) for proper sampling
- Result: realistic financial time series

STEP 5: EVALUATION
- CRPS: measures distributional accuracy
- Distance metrics: compare statistical properties
- Visual inspection: qualitative assessment
- Comprehensive assessment beyond simple MSE

WHAT MAKES THIS DIFFERENT FROM STANDARD ML:

Traditional ML: learns mapping from input to output
Diffusion: learns to gradually transform noise into data

Benefits for finance:
- Generates diverse, realistic scenarios
- Captures uncertainty naturally
- Can condition on economic variables
- Preserves complex statistical properties

PRACTICAL APPLICATIONS:

1. Risk Management:
   - Generate stress test scenarios
   - Monte Carlo simulations
   - Tail risk assessment

2. Portfolio Optimization:
   - Synthetic data augmentation
   - Backtesting with more data
   - Regime change modeling

3. Derivatives Pricing:
   - Path-dependent option pricing
   - Exotic derivatives valuation
   - Market microstructure modeling

4. Research and Development:
   - Test strategies on synthetic data
   - Explore counterfactual scenarios
   - Validate models with unlimited data

NEXT STEPS FOR ENHANCEMENT:

1. Conditioning:
   - Add macroeconomic variables (interest rates, GDP, etc.)
   - Sector-specific conditioning
   - Volatility regime conditioning

2. Architecture Improvements:
   - Wavelet preprocessing for multi-scale analysis
   - Attention mechanisms for long-range dependencies
   - KAN (Kolmogorov-Arnold Networks) for interpretability

3. Training Enhancements:
   - Progressive training schedules
   - Classifier-free guidance
   - Score-based formulations

4. Evaluation Extensions:
   - Stylized facts preservation
   - Economic significance testing
   - Out-of-sample validation

The model you've trained represents a sophisticated approach to financial
data generation that goes far beyond traditional statistical methods.
It captures the complex, multi-scale nature of financial markets while
providing a principled framework for uncertainty quantification.
"""

# ============================================================================
# TECHNICAL PERFORMANCE NOTES
# ============================================================================
"""
TRAINING PERFORMANCE OPTIMIZATION:

Device Selection:
- GPU training: ~10x faster than CPU for this model size
- Mixed precision: can provide additional 1.5-2x speedup
- Multiple GPUs: model is small enough for single GPU

Memory Usage:
- Model parameters: ~1M parameters â‰ˆ 4MB
- Batch processing: 128 sequences Ã— 60 timesteps Ã— 4 features â‰ˆ 1.2MB per batch
- Gradient computation: 2x model size during backprop â‰ˆ 8MB
- Total GPU memory: ~50-100MB (very modest requirements)

Training Time Estimates:
- CPU: ~2-3 hours for 100 epochs with this dataset size
- GPU (RTX 3080): ~15-20 minutes for 100 epochs
- CPU + mixed precision: ~1.5-2 hours
- GPU + mixed precision: ~10-12 minutes

Convergence Characteristics:
- Loss typically decreases rapidly in first 10-20 epochs
- Generated samples become visually realistic around epoch 30-50
- Statistical properties converge around epoch 70-100
- Overtraining rarely a problem due to stochastic sampling

HYPERPARAMETER SENSITIVITY:

Critical parameters:
- Learning rate (0.001): too high causes instability, too low slows convergence
- Sequence length (60): longer sequences need more model capacity
- Number of layers (10): more layers = larger receptive field but slower training
- Time steps (300): more steps = higher quality but slower sampling

Less critical parameters:
- Batch size (128): mainly affects training speed, not quality
- Hidden dimensions (64): affects model capacity vs speed tradeoff
- Beta schedule: linear works well, cosine can be slightly better

QUALITY ASSESSMENT:

Good signs:
- Loss decreases steadily without plateauing early
- Generated samples look realistic when plotted
- No obvious artifacts (spikes, discontinuities, unrealistic values)
- Statistical properties match real data

Warning signs:
- Loss oscillates wildly: learning rate too high
- Generated samples all look similar: mode collapse
- Extreme values in generated data: numerical instability
- Poor CRPS scores: model not learning proper distributions

This implementation provides a solid foundation that can be extended
for more sophisticated financial modeling applications.
"""